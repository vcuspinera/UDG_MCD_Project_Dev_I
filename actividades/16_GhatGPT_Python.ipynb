{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e09b550-d4e0-4193-9b02-fbe7577c52aa",
   "metadata": {},
   "source": [
    "# 20 • Uso de ChatGPT con Python\n",
    "\n",
    "## ⚠️ Actividad en revisión\n",
    "\n",
    "ChatGPT es una aplicación de chatbot de inteligencia artificial desarrollado por OpenAI que se especializa en el diálogo. La página de esta herramienta es: https://platform.openai.com/docs/overview\n",
    "\n",
    "Para poder utilizar ChatGPT desde Python es necesario seguir los siguientes pasos de acuerdo a [**OpenAI Python API library**](https://github.com/openai/openai-python):\n",
    "- Instalarlo con `pip install openai`.\n",
    "- Suscribirse a OpenAI y obtener una llave en la API oficial.\n",
    "- Seguir la sección de `Usage` de la siguiente liga: https://github.com/openai/openai-python#usage\n",
    "\n",
    "\n",
    "## Contenido\n",
    "1. Instalación\n",
    "2. Hacer una Pregunta\n",
    "3. Solicitar ejemplo de código\n",
    "4. Referencias  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8eb864-51e2-49f3-bcec-db1a1db2866d",
   "metadata": {},
   "source": [
    "### 1. Instalación de OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4255275-a439-4fa9-a4a3-a0451f53bafa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Instalación de OpenAI en Python\n",
    "# ! pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9900f4e9-78d9-49d9-9872-ba9398bf6585",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# llamar librerías\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4bd1d053-11e7-4751-9c19-fa8f56f14b42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# poner la llave personal de OpenAI\n",
    "file1 = open('../../09_Pswd/ChatGPT_vic.txt',\"r\")\n",
    "my_key = file1.read().split(\"\\n\")[1]\n",
    "file1.close()\n",
    "\n",
    "# Usar la llave\n",
    "client = OpenAI(\n",
    "    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n",
    "    api_key=my_key\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c810f7b-4429-400f-9073-e175ef99780e",
   "metadata": {},
   "source": [
    "### 2. Hacer una pregunta\n",
    "En esta sección se hizo una pregunta para ver la respuesta que daba; en específico, se preguntó en español ¿cuál es la mejor forma de instalar Python?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "399270a6-aab0-4786-b06b-10bf69d24ea5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:1\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/_utils/_utils.py:301\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    299\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 301\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/resources/chat/completions.py:598\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    552\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    596\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    597\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 598\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m    599\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    600\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[1;32m    601\u001b[0m             {\n\u001b[1;32m    602\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[1;32m    603\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m    604\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[1;32m    605\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[1;32m    606\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[1;32m    607\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[1;32m    608\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[1;32m    609\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[1;32m    610\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[1;32m    611\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[1;32m    612\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[1;32m    613\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[1;32m    614\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[1;32m    615\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m    616\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[1;32m    617\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[1;32m    618\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[1;32m    619\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[1;32m    620\u001b[0m             },\n\u001b[1;32m    621\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[1;32m    622\u001b[0m         ),\n\u001b[1;32m    623\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m    624\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m    625\u001b[0m         ),\n\u001b[1;32m    626\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[1;32m    627\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    628\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mStream[ChatCompletionChunk],\n\u001b[1;32m    629\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:1096\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1082\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1083\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1084\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1091\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1092\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1093\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1094\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1095\u001b[0m     )\n\u001b[0;32m-> 1096\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:856\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    847\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    848\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    849\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    854\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    855\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 856\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m    857\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m    858\u001b[0m         options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    859\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    860\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    861\u001b[0m         remaining_retries\u001b[38;5;241m=\u001b[39mremaining_retries,\n\u001b[1;32m    862\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:894\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m    893\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 894\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[1;32m    895\u001b[0m         options,\n\u001b[1;32m    896\u001b[0m         cast_to,\n\u001b[1;32m    897\u001b[0m         retries,\n\u001b[1;32m    898\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    899\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    900\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    901\u001b[0m     )\n\u001b[1;32m    903\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m    904\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:966\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m    962\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m    963\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m    964\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m--> 966\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m    967\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    968\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m    969\u001b[0m     remaining_retries\u001b[38;5;241m=\u001b[39mremaining,\n\u001b[1;32m    970\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    971\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    972\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:894\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m    893\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 894\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[1;32m    895\u001b[0m         options,\n\u001b[1;32m    896\u001b[0m         cast_to,\n\u001b[1;32m    897\u001b[0m         retries,\n\u001b[1;32m    898\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    899\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    900\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    901\u001b[0m     )\n\u001b[1;32m    903\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m    904\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:966\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m    962\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m    963\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m    964\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m--> 966\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m    967\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    968\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m    969\u001b[0m     remaining_retries\u001b[38;5;241m=\u001b[39mremaining,\n\u001b[1;32m    970\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    971\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    972\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:908\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    905\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[1;32m    906\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m--> 908\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    909\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    910\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"¿Cuál es la mejor forma de instalar Python?\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-3.5-turbo\",\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce702d37-f9dc-40ec-9f71-ee49ce20bf97",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='La mejor forma de instalar Python depende del sistema operativo que estés utilizando. A continuación, te menciono algunas opciones comunes:\n",
      "\n",
      "1. Windows: Puedes descargar el instalador oficial de Python desde el sitio web oficial de Python (https://www.python.org). Simplemente descarga el archivo ejecutable y sigue las instrucciones para instalar Python en tu sistema.\n",
      "\n",
      "2. macOS: La forma más sencilla de instalar Python en macOS es utilizar Homebrew, un gestor de paquetes de código abierto para macOS. Abre la terminal y ejecuta el siguiente comando para instalar Homebrew:\n",
      "```\n",
      "/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n",
      "```\n",
      "Después de instalar Homebrew, puedes ejecutar el siguiente comando para instalar Python:\n",
      "```\n",
      "brew install python\n",
      "```\n",
      "\n",
      "3. Linux: La mayoría de las distribuciones de Linux vienen con Python preinstalado. Sin embargo, si necesitas una versión específica o quieres realizar una instalación personalizada, puedes utilizar el gestor de paquetes de tu distribución para instalar Python. Por ejemplo, en Debian o Ubuntu, puedes ejecutar el siguiente comando en la terminal:\n",
      "```\n",
      "sudo apt-get install python3\n",
      "```\n",
      "Este comando instalará Python 3.x, que es la versión recomendada en la actualidad.\n",
      "\n",
      "Adicionalmente, también puedes utilizar herramientas como Anaconda o Miniconda para instalar y gestionar Python y sus paquetes en tu sistema.\n",
      "\n",
      "Recuerda que antes de instalar Python, debes comprobar si ya está instalado en tu sistema y la versión correspondiente. Para ello, puedes ejecutar `python --version` o `python3 --version` en la terminal, dependiendo de tu sistema operativo.', role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "# imprimirlo con salto de renglón\n",
    "for i in str(completion.choices[0].message).split(\"\\\\n\"):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4e86a8-c5b7-4620-93c3-43130b643b1d",
   "metadata": {},
   "source": [
    "### 3. Solicitar ejemplo de código\n",
    "En esta sección se pidió a ChatGPT escribir un programa de Python con utilizando la librería spaCy, y se recibió una respuesta con un ejemplo de cómo utilizar la librerías de spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbf3c252-5dcf-48d9-ad94-86309aa63b74",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='Claro, aquí tienes un ejemplo de un programa de Python que utiliza la librería spaCy:\\n\\n```python\\nimport spacy\\n\\n# Cargar el modelo de lenguaje en español\\nnlp = spacy.load(\"es_core_news_sm\")\\n\\n# Texto de ejemplo\\ntexto = \"La inteligencia artificial es una rama de la informática que se encarga de crear programas y máquinas capaces de simular el pensamiento humano.\"\\n\\n# Procesar el texto con spaCy\\ndoc = nlp(texto)\\n\\n# Imprimir las entidades nombradas\\nprint(\"Entidades nombradas:\")\\nfor entity in doc.ents:\\n    print(entity.text, entity.label_)\\n\\n# Imprimir las oraciones\\nprint(\"\\\\nOraciones:\")\\nfor sentence in doc.sents:\\n    print(sentence)\\n\\n# Imprimir los tokens con sus atributos\\nprint(\"\\\\nTokens:\")\\nfor token in doc:\\n    print(token.text, token.pos_, token.dep_)\\n```\\n\\nEn este ejemplo, estamos utilizando la librería spaCy para realizar diferentes tareas de procesamiento de lenguaje natural. Primero, cargamos el modelo de lenguaje en español utilizando `spacy.load(\"es_core_news_sm\")`. Luego, procesamos un texto de ejemplo utilizando el modelo cargado y lo almacenamos en la variable `doc`.\\n\\nDespués, imprimimos las entidades nombradas encontradas en el texto utilizando `doc.ents` y el atributo `text` y `label_`. A continuación, imprimimos las oraciones utilizando `doc.sents` y finalmente, imprimimos los tokens del texto con sus atributos utilizando un bucle `for` y accediendo a los atributos `text`, `pos_` y `dep_`.\\n\\nEspero que esto te sea de ayuda para empezar a utilizar la librería spaCy en tus programas de Python. ¡Si tienes alguna pregunta, no dudes en hacerla!', role='assistant', function_call=None, tool_calls=None) \n",
      "\n",
      "CPU times: user 9.5 ms, sys: 3.33 ms, total: 12.8 ms\n",
      "Wall time: 1min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Escribe un programa de Python con utilice la librería spaCy\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-3.5-turbo\",\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30fdd864-ab8f-46df-be1f-baa56a73adb6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='Claro, aquí tienes un ejemplo de un programa de Python que utiliza la librería spaCy:\n",
      "\n",
      "```python\n",
      "import spacy\n",
      "\n",
      "# Cargar el modelo de lenguaje en español\n",
      "nlp = spacy.load(\"es_core_news_sm\")\n",
      "\n",
      "# Texto de ejemplo\n",
      "texto = \"La inteligencia artificial es una rama de la informática que se encarga de crear programas y máquinas capaces de simular el pensamiento humano.\"\n",
      "\n",
      "# Procesar el texto con spaCy\n",
      "doc = nlp(texto)\n",
      "\n",
      "# Imprimir las entidades nombradas\n",
      "print(\"Entidades nombradas:\")\n",
      "for entity in doc.ents:\n",
      "    print(entity.text, entity.label_)\n",
      "\n",
      "# Imprimir las oraciones\n",
      "print(\"\\\n",
      "Oraciones:\")\n",
      "for sentence in doc.sents:\n",
      "    print(sentence)\n",
      "\n",
      "# Imprimir los tokens con sus atributos\n",
      "print(\"\\\n",
      "Tokens:\")\n",
      "for token in doc:\n",
      "    print(token.text, token.pos_, token.dep_)\n",
      "```\n",
      "\n",
      "En este ejemplo, estamos utilizando la librería spaCy para realizar diferentes tareas de procesamiento de lenguaje natural. Primero, cargamos el modelo de lenguaje en español utilizando `spacy.load(\"es_core_news_sm\")`. Luego, procesamos un texto de ejemplo utilizando el modelo cargado y lo almacenamos en la variable `doc`.\n",
      "\n",
      "Después, imprimimos las entidades nombradas encontradas en el texto utilizando `doc.ents` y el atributo `text` y `label_`. A continuación, imprimimos las oraciones utilizando `doc.sents` y finalmente, imprimimos los tokens del texto con sus atributos utilizando un bucle `for` y accediendo a los atributos `text`, `pos_` y `dep_`.\n",
      "\n",
      "Espero que esto te sea de ayuda para empezar a utilizar la librería spaCy en tus programas de Python. ¡Si tienes alguna pregunta, no dudes en hacerla!', role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "# imprimirlo con salto de renglón\n",
    "for i in str(completion.choices[0].message).split(\"\\\\n\"):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18b9ee9-a23a-44d2-88e8-28f31fa4009b",
   "metadata": {},
   "source": [
    "### 4. Referencias\n",
    "- Plataforma **ChatGPT** desarrollada por OpenAI: https://platform.openai.com/docs/overview\n",
    "- Librería `openai-python`: https://github.com/openai/openai-python\n",
    "- Ejemplo de uso de la librería por parte de [Alejandro-Tecno](https://github.com/Alejandro-Tecno/GPT_first_steps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc6aa69-0a86-4c6c-b6bc-1e71044d778c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
